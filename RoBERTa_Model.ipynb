{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To run this python book, one may need to install dependcies like, transformers, seaborn. Also, since the google colab deletes all the files after the runtime is close, one needs to upload the dataset here. It's availble in my github repository."
      ],
      "metadata": {
        "id": "Q7Vr85mcnGUj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_JKmVevHbZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc6d2c0-a50f-4077-a1f9-9e8acc81a13a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizerFast\n",
        "from transformers import RobertaForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv('dataset.csv')\n",
        "# dataset = dataset.head(1000)\n",
        "\n",
        "dataset['sentiment'] = dataset['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load Roberta tokenizer\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "\n",
        "# Tokenize input text and create dataset\n",
        "train_encodings = tokenizer(list(train_df['review']), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(test_df['review']), truncation=True, padding=True)\n",
        "\n",
        "train_labels = list(train_df['sentiment'])\n",
        "test_labels = list(test_df['sentiment'])\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
        "                              torch.tensor(train_encodings['attention_mask']),\n",
        "                              torch.tensor(train_labels))\n",
        "\n",
        "test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
        "                             torch.tensor(test_encodings['attention_mask']),\n",
        "                             torch.tensor(test_labels))\n",
        "\n",
        "# Load Roberta model and set up training parameters\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Train model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate model on test set and calculate F1 score\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[0]\n",
        "\n",
        "    y_true.extend(labels.tolist())\n",
        "    y_pred.extend(torch.argmax(logits, axis=1).tolist())\n",
        "\n",
        "\n",
        "y_test = y_true \n",
        "y_pred_test = y_pred\n",
        "\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "f1_test = f1_score(y_test, y_pred_test, pos_label=1)\n",
        "precision_test = precision_score(y_test, y_pred_test, pos_label=1)\n",
        "recall_test = recall_score(y_test, y_pred_test, pos_label=1)\n",
        "confusion_test = confusion_matrix(y_test, y_pred_test)\n",
        "classification_report_test = classification_report(y_test, y_pred_test)\n",
        "\n",
        "\n",
        "\n",
        "print(f'RoBERTa Testing accuracy: {accuracy_test:.4f}')\n",
        "print(f'RoBERTa Testing F1 score: {f1_test:.4f}')\n",
        "print(f'RoBERTa Testing precision: {precision_test:.4f}')\n",
        "print(f'RoBERTa Testing recall: {recall_test:.4f}')\n",
        "print(f'RoBERTa Testing confusion matrix:\\n{confusion_test}')\n",
        "print('RoBERTa Classification Report:\\n', classification_report_test)\n",
        "\n",
        "\n",
        "# Plot confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "sns.heatmap(confusion_test, annot=True, fmt='g', cmap='Blues', cbar=False,\n",
        "            xticklabels=['negative', 'positive'], yticklabels=['negative', 'positive'])\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.set_xlabel('Predicted Label')\n",
        "ax.set_ylabel('True Label')\n",
        "plt.show()\n"
      ]
    }
  ]
}